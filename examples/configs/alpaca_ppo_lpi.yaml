# LPI (Local Parameter Importance) Analysis for PPO
# Run AFTER HPO completes to analyze which HPs matter most around the incumbent
#
# Usage:
#   python examples/hpo_alpaca_vecenv.py -m --config-name=alpaca_ppo_lpi \
#       hydra.sweeper.sweeper_kwargs.data_path=outputs/ppo_sweep/<timestamp>/runhistory.csv
#
# Output:
#   - lpi_scores.csv: Importance scores per hyperparameter
#   - lpi_plot.png: Bar chart visualization

defaults:
  - _self_
  - search_space: ppo_space
  - override hydra/sweeper: HyperLPI

# Agent settings (same as main config)
agent: ppo
gpu_id: 0

# Training settings for evaluation runs
seed: 42
break_step: 100000  # Shorter runs for analysis

# Environment settings
state_dim: 283
action_dim: 28
max_step: 119
num_envs: 2048

# Default hyperparameters (will be varied around incumbent)
learning_rate: 1e-4
gamma: 0.99
net_arch: "small"
batch_size: 512
repeat_times: 16
ratio_clip: 0.25
lambda_gae_adv: 0.95
lambda_entropy: 0.01
clip_grad_norm: 3.0
if_use_v_trace: true

# VecNormalize
use_vec_normalize: true
norm_obs: true
norm_reward: false  # On-policy: env already scales rewards (2^-12); double-normalization causes GAE divergence

# CV settings (single holdout for analysis)
cv_method: holdout
n_folds: 1
n_groups: 5
n_test_groups: 2
embargo_pct: 0.01
test_ratio: 0.2
gap_days: 0

hydra:
  sweeper:
    sweeper_kwargs:
      # Path to runhistory from HPO (REQUIRED - override on command line)
      data_path: null  # outputs/ppo_sweep/<timestamp>/runhistory.csv
      performance_key: "performance"
      config_key: "config_id"
      configs_per_hp: 10  # Number of values to test per HP
      run_source: true  # Also evaluate the incumbent config
  run:
    dir: outputs/${agent}_lpi/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: outputs/${agent}_lpi/${now:%Y-%m-%d_%H-%M-%S}
